#!/bin/bash

# see https://github.com/ollama/ollama/blob/main/docs/faq.md

source $(dirname $0)/echo_formatted.sh

# if no arguments then print usage
if [ -z "$1" ]; then
  echo "Usage: ollama-run.sh [model] <prompt>"
  exit 1
fi

if [ -z "$2" ]; then
  model="tinyllama"
  prompt="$1"
else
  model="$1"
  prompt="$2"
fi

# set OLLAMA_HOST if not defined
[ -z "$OLLAMA_HOST" ] && OLLAMA_HOST=localhost:11434

if ! curl -s http://$OLLAMA_HOST/api/tags | jq -r '.models[].model' | grep -q $model; then
	echo_italics "  skipping $model because it is not available (model not pulled or service unavailable)" >&2
  echo 
  exit 1
fi

# ollama API
# curl -s http://$OLLAMA_HOST/api/generate \
#  --data "$( jq -cn '{  "model": $ARGS.positional[0], stream: false, options: {seed: 0, temperature: 0}, "prompt": $ARGS.positional[1]   }' --args "$model" "$prompt" )" \
# | jq -r .response

# echo

# read extra parameters from file (if any)
if [ -f "ai-extra-params.json" ]; then
  extra_params=$(cat "ai-extra-params.json")
else
  extra_params="{}"
fi

# openai-like API
curl -s http://$OLLAMA_HOST/v1/chat/completions \
  --header 'Content-Type: application/json' \
  --data "$( jq -cn '{  "model": $ARGS.positional[0], "messages": [ { "role": "user", "content": $ARGS.positional[1] } ]  } + ($ARGS.positional[2] | fromjson )' --args "$model" "$prompt" "$extra_params" )" \
| jq -r '.choices[].message.content'

# echo an empty line to be compatible with ollama CLI output and also to separate responses
echo
